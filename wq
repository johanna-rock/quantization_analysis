#!/usr/bin/env python3
from __future__ import annotations

import argparse
import csv
import json
import os
import re
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
import secrets

import numpy as np
from tqdm import tqdm

from compression_algorithms import create_algorithm
from compression_algorithms.cache import CacheContext
from compression_algorithms.config import load_compression_config
from compression_algorithms.metrics import pearson_corr
from compression_algorithms.quantizer import Quantizer
from compression_algorithms.tile_utils import MIXED_TILE_FORMATS
from hf_model_utils import (
    _safe_repo_revision_key,
    _safe_tensor_key,
    build_model_index,
    fp32_tensor_cache_dir,
    load_tensor_fp32,
    resolve_format_list,
    resolve_selected_tensors,
)
from quantization_formats import SUPPORTED_FORMATS


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        prog="wq",
        description="Weight quantization analyzer for Hugging Face checkpoints.",
    )
    parser.add_argument("repo_or_url", help="Hugging Face model repo/URL.")
    parser.add_argument(
        "filter_query",
        nargs="*",
        help="Optional filter: substring, or dotted torch-style prefix path.",
    )
    parser.add_argument("--revision", default="main", help="Hugging Face revision (default: main).")
    parser.add_argument(
        "--cache-dir",
        default="data/hf-cache",
        help="Shared local cache for downloaded/dequantized tensors (default: data/hf-cache).",
    )
    parser.add_argument(
        "--limit", type=int, default=None, help="Optional max matched tensors."
    )
    parser.add_argument(
        "--backend",
        choices=["emulation", "ttnn"],
        default="emulation",
        help="Quantization backend for BFP formats (default: emulation).",
    )
    parser.add_argument(
        "--compression-config",
        type=str,
        default=None,
        help="Path to a JSON compression config file (default: none).",
    )
    parser.add_argument(
        "--recompute",
        action="store_true",
        help="Recompute and overwrite cached quantized tensors.",
    )
    parser.add_argument(
        "--summary",
        action="store_true",
        help="Print the aggregate summary (default: off).",
    )
    return parser.parse_args()


def _tensor_meta_str(x: np.ndarray) -> str:
    x = np.asarray(x, dtype=np.float32)
    return f"shape={tuple(x.shape)} min={np.min(x):.3e} mean={np.mean(x):.3e} max={np.max(x):.3e}"


COLOR_ENABLED = sys.stdout.isatty() and os.getenv("TERM", "") != "dumb" and not os.getenv("NO_COLOR")
COLORS = {
    "reset": "\033[0m",
    "title": "\033[1;37m",
    "muted": "\033[90m",
    "good": "\033[92m",
    "mid": "\033[93m",
    "bad": "\033[91m",
    "cyan": "\033[96m",
}


def paint(text: str, color: str) -> str:
    if not COLOR_ENABLED:
        return text
    return f"{COLORS[color]}{text}{COLORS['reset']}"


ANSI_RE = re.compile(r"\x1b\[[0-9;]*m")


def strip_ansi(text: str) -> str:
    return ANSI_RE.sub("", text)


def _slug(s: str) -> str:
    return re.sub(r"[^a-zA-Z0-9._-]+", "_", s).strip("_") or "tensor"


def color_pcc(v: float) -> str:
    if v >= 0.999:
        return "good"
    if v >= 0.99:
        return "mid"
    return "bad"


def pcc_plot_color(v: float) -> str:
    bucket = color_pcc(v)
    if bucket == "good":
        return "#2ca02c"
    if bucket == "mid":
        return "#ffbf00"
    return "#d62728"


def color_err(v: float) -> str:
    if v <= 1e-4:
        return "good"
    if v <= 1e-3:
        return "mid"
    return "bad"


def _write_mixed_tile_random_outputs(
    out_dir: Path,
    tensor_name: str,
    samples: list[dict],
    tile_formats: list[str],
    assignment: np.ndarray | None,
) -> None:
    if not samples:
        return

    mt_dir = out_dir / "mixed_tile_random"
    mt_dir.mkdir(parents=True, exist_ok=True)
    slug = _slug(tensor_name)

    csv_path = mt_dir / f"{slug}.csv"
    headers = ["sample_id", *[f"{fmt}_tiles" for fmt in tile_formats], "total_gb", "pcc", "mae", "atol"]
    with csv_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        for sample in samples:
            counts = sample.get("counts", {})
            total_gb = float(sample.get("total_bytes", 0.0)) / 1e9
            row = [
                sample.get("id"),
                *[counts.get(fmt, 0) for fmt in tile_formats],
                total_gb,
                sample.get("pcc"),
                sample.get("mae"),
                sample.get("atol"),
            ]
            writer.writerow(row)

    if assignment is not None:
        assign_path = mt_dir / f"{slug}_assignment.npy"
        np.save(assign_path, assignment.astype(np.int8))
        mapping_path = mt_dir / f"{slug}_assignment_mapping.json"
        mapping = {
            "tile_hw": 32,
            "format_to_int": {fmt: idx for idx, fmt in enumerate(MIXED_TILE_FORMATS)},
            "int_to_format": MIXED_TILE_FORMATS,
            "assignment_shape": list(assignment.shape),
        }
        with mapping_path.open("w", encoding="utf-8") as f:
            json.dump(mapping, f, indent=2)

    try:
        import matplotlib

        matplotlib.use("Agg")
        import matplotlib.pyplot as plt
    except Exception:
        return

    xs = [float(sample.get("pcc", 0.0)) for sample in samples]
    ys = [float(sample.get("total_bytes", 0.0)) / 1e9 for sample in samples]
    colors = [pcc_plot_color(x) for x in xs]

    fig, ax = plt.subplots(figsize=(6.5, 4.5))
    ax.scatter(xs, ys, c=colors, s=28, alpha=0.9)
    for sample, x, y in zip(samples, xs, ys):
        ax.annotate(str(sample.get("id")), (x, y), textcoords="offset points", xytext=(4, 4), fontsize=7)
    ax.set_xlabel("PCC")
    ax.set_ylabel("Total size (GB)")
    ax.set_title("Mixed-tile random samples")
    ax.grid(True, alpha=0.3)
    fig.tight_layout()
    fig.savefig(mt_dir / f"{slug}.png", dpi=160)
    plt.close(fig)


@dataclass
class Row:
    fmt: str
    compression: str
    pcc: float
    mae: float
    atol: float
    tile_counts: dict[str, int] | None = None
    tile_bytes: float | None = None


def _import_ttnn_quiet():
    os.environ.setdefault("LOGURU_LEVEL", "ERROR")
    os.environ.setdefault("TTNN_LOG_LEVEL", "ERROR")
    os.environ.setdefault("TT_METAL_LOGGER_LEVEL", "ERROR")
    try:
        import ttnn  # pylint: disable=import-outside-toplevel
    except ModuleNotFoundError as exc:
        raise RuntimeError("TTNN backend requires `ttnn` in the active Python environment.") from exc
    return ttnn


def _build_hierarchy(tensor_names: list[str]) -> dict:
    root: dict = {}
    for name in sorted(tensor_names):
        node = root
        for part in name.split("."):
            node = node.setdefault(part, {})
    return root


def _count_leaves(node: dict) -> int:
    if not node:
        return 1
    return sum(_count_leaves(child) for child in node.values())


def _render_hierarchy_lines(node: dict, prefix: str = "") -> list[str]:
    lines: list[str] = []
    items = sorted(node.items(), key=lambda kv: kv[0])
    for i, (name, child) in enumerate(items):
        is_last = i == len(items) - 1
        branch = "└── " if is_last else "├── "
        count = _count_leaves(child)
        label = f"{name} {paint(f'({count})', 'muted') if count > 1 else ''}".rstrip()
        lines.append(f"{prefix}{branch}{label}")
        if child:
            ext = "    " if is_last else "│   "
            lines.extend(_render_hierarchy_lines(child, prefix + ext))
    return lines


def _print_hierarchy(tensor_names: list[str]) -> None:
    tree = _build_hierarchy(tensor_names)
    print(paint("Hierarchy", "title"))
    for line in _render_hierarchy_lines(tree):
        print(f"  {paint(line, 'muted')}")
    print()


def run() -> int:
    args = parse_args()
    run_tag = datetime.now().strftime("%Y%m%d-%H%M%S")

    config = load_compression_config(args.compression_config)
    algo_params = dict(config.params)
    seed_source = None
    if config.seed is not None:
        used_seed = config.seed
        seed_source = "config"
    elif config.random_seed:
        used_seed = secrets.randbits(31)
        seed_source = "random"
    else:
        used_seed = None

    if used_seed is not None:
        algo_params["seed"] = used_seed
    elif "seed" in algo_params:
        used_seed = algo_params["seed"]
        seed_source = "params"

    selected_algo = create_algorithm(config.algorithm, algo_params)
    baseline = create_algorithm("none", {})
    algorithms = [baseline] if selected_algo.name == "none" else [baseline, selected_algo]

    filter_query = " ".join(args.filter_query).strip() or None
    formats = resolve_format_list(config.quantization_formats, SUPPORTED_FORMATS)

    index = build_model_index(repo_or_url=args.repo_or_url, revision=args.revision, cache_dir=args.cache_dir)
    tensor_names = resolve_selected_tensors(index, filter_query)
    if args.limit is not None:
        tensor_names = tensor_names[: max(0, args.limit)]
    if not tensor_names:
        print("No tensors matched.", file=sys.stderr)
        return 1

    ttnn = None
    if args.backend == "ttnn":
        try:
            ttnn = _import_ttnn_quiet()
        except Exception as exc:
            print(f"error: {exc}", file=sys.stderr)
            return 1

    quantizer = Quantizer(backend=args.backend, ttnn=ttnn)

    compression_names = [algo.name for algo in algorithms]
    comp_w = max(len("COMP"), max((len(name) for name in compression_names), default=0))
    results_dir = None
    table_lines: list[str] = []
    print(
        f"{paint(index.repo_id, 'title')} {paint('@', 'muted')}{paint(index.revision, 'cyan')} "
        f"{paint('-', 'muted')} {paint(str(len(tensor_names)), 'title')} {paint('tensors', 'muted')}"
    )
    print(f"{paint('formats:', 'muted')} {', '.join(formats)}")
    print(f"{paint('compression:', 'muted')} {', '.join(compression_names)}")
    print(f"{paint('backend:', 'muted')} {args.backend}")
    if args.compression_config:
        print(f"{paint('config:', 'muted')} {args.compression_config}")
    print()
    _print_hierarchy(tensor_names)

    safe_model = index.repo_id.replace("/", "__")
    results_dir = Path("results") / safe_model / selected_algo.name / run_tag
    results_dir.mkdir(parents=True, exist_ok=True)
    used_params = dict(algo_params)
    if used_seed is not None and "seed" in used_params:
        used_params.pop("seed", None)

    used_config = {
        "algorithm": config.algorithm,
        "quantization_formats": formats,
        "params": used_params,
    }
    if used_seed is not None:
        used_config["seed"] = used_seed
        if seed_source:
            used_config["seed_source"] = seed_source

    with (results_dir / "compression_config.used.json").open("w", encoding="utf-8") as f:
        json.dump(used_config, f, indent=2)

    processed_root = Path("data/processed") / _safe_repo_revision_key(index.repo_id, index.revision)
    aggregate: dict[tuple[str, str], list[Row]] = {}

    total_evals = len(tensor_names) * sum(algo.expected_evals(formats) for algo in algorithms)
    pbar = tqdm(total=total_evals, desc="Evaluating", unit="eval")

    for tensor_name in tensor_names:
        cache_file = fp32_tensor_cache_dir(index) / f"{_safe_tensor_key(tensor_name)}.npy"
        if cache_file.exists():
            print(f"{paint('cache:', 'muted')} fp32 hit ({cache_file})")
        else:
            print(f"{paint('cache:', 'muted')} fp32 miss -> loading from HF cache/download")
        x = load_tensor_fp32(index, tensor_name)
        xf = np.asarray(x, dtype=np.float32)
        print(paint(tensor_name, "title"))
        print(f"  {paint(_tensor_meta_str(xf), 'muted')}")

        cache_ctx = CacheContext(
            root=processed_root,
            tensor_name=tensor_name,
            backend=args.backend,
            recompute=args.recompute,
            run_tag=run_tag,
        )

        rows_by_comp: dict[str, list[Row]] = {}
        for algo in algorithms:
            results = algo.run(xf=xf, formats=formats, quantizer=quantizer, cache=cache_ctx)
            for res in results:
                diff = np.abs(xf - res.y)
                mae = float(np.mean(diff))
                atol = float(np.max(diff))
                pcc = pearson_corr(xf, res.y)
                row = Row(
                    fmt=res.fmt,
                    compression=res.compression,
                    pcc=pcc,
                    mae=mae,
                    atol=atol,
                    tile_counts=res.tile_counts,
                    tile_bytes=res.tile_bytes,
                )
                rows_by_comp.setdefault(res.compression, []).append(row)
                aggregate.setdefault((res.compression, res.fmt), []).append(row)
                pbar.update(1)

                if (
                    res.compression == "mixed-tile-random"
                    and res.meta
                    and results_dir is not None
                ):
                    samples = res.meta.get("samples")
                    tile_formats = res.meta.get("tile_formats", [])
                    assignment = res.meta.get("assignment")
                    if isinstance(samples, list) and tile_formats:
                        _write_mixed_tile_random_outputs(
                            results_dir, tensor_name, samples, tile_formats, assignment
                        )

        for comp in compression_names:
            rows = rows_by_comp.get(comp, [])
            if not rows:
                continue
            fmt_w = max(len(r.fmt) for r in rows)
            if comp in {"mixed-tile-greedy", "mixed-tile-random"}:
                count_widths = {k: len(k.upper()) for k in MIXED_TILE_FORMATS}
                bytes_w = len("BYTES")
                for r in rows:
                    counts = r.tile_counts or {}
                    for k in MIXED_TILE_FORMATS:
                        count_widths[k] = max(count_widths[k], len(str(counts.get(k, 0))))
                    if r.tile_bytes is not None:
                        bytes_w = max(bytes_w, len(f"{r.tile_bytes:,.0f}"))
                count_hdr = "  ".join(k.upper().rjust(count_widths[k]) for k in MIXED_TILE_FORMATS)
                header = (
                    f"  {paint('COMP'.ljust(comp_w), 'muted')}  {paint('FORMAT'.ljust(fmt_w), 'muted')}  "
                    f"{paint('PCC', 'muted')}      {paint('MAE', 'muted')}      {paint('ATOL', 'muted')}  "
                    f"{paint(count_hdr, 'muted')}  {paint('BYTES'.rjust(bytes_w), 'muted')}"
                )
                print(header)
                table_lines.append(strip_ansi(header))
                for r in rows:
                    pcc_txt = f"{r.pcc: .5f}"
                    mae_txt = f"{r.mae:.3e}"
                    atol_txt = f"{r.atol:.3e}"
                    counts = r.tile_counts or {}
                    counts_txt = "  ".join(str(counts.get(k, 0)).rjust(count_widths[k]) for k in MIXED_TILE_FORMATS)
                    bytes_txt = f"{(r.tile_bytes or 0.0):,.0f}".rjust(bytes_w)
                    line = (
                        f"  {r.compression.ljust(comp_w)}  {r.fmt.ljust(fmt_w)}  "
                        f"{paint(pcc_txt, color_pcc(r.pcc))}  "
                        f"{paint(mae_txt, color_err(r.mae))}  "
                        f"{paint(atol_txt, color_err(r.atol))}  "
                        f"{counts_txt}  {bytes_txt}"
                    )
                    print(line)
                    table_lines.append(strip_ansi(line))
            else:
                header = (
                    f"  {paint('COMP'.ljust(comp_w), 'muted')}  {paint('FORMAT'.ljust(fmt_w), 'muted')}  "
                    f"{paint('PCC', 'muted')}      {paint('MAE', 'muted')}      {paint('ATOL', 'muted')}"
                )
                print(header)
                table_lines.append(strip_ansi(header))
                for r in rows:
                    pcc_txt = f"{r.pcc: .5f}"
                    mae_txt = f"{r.mae:.3e}"
                    atol_txt = f"{r.atol:.3e}"
                    line = (
                        f"  {r.compression.ljust(comp_w)}  {r.fmt.ljust(fmt_w)}  "
                        f"{paint(pcc_txt, color_pcc(r.pcc))}  "
                        f"{paint(mae_txt, color_err(r.mae))}  "
                        f"{paint(atol_txt, color_err(r.atol))}"
                    )
                    print(line)
                    table_lines.append(strip_ansi(line))
            print()
            table_lines.append("")

    pbar.close()
    if args.summary:
        print(paint("Summary (mean across matched tensors)", "title"))
        table_lines.append("Summary (mean across matched tensors)")
        for comp in compression_names:
            if comp in {"mixed-tile-greedy", "mixed-tile-random"}:
                fmt_list = ["MIXED"]
            else:
                fmt_list = [fmt.upper() for fmt in formats]
            for fmt in fmt_list:
                rows = aggregate.get((comp, fmt), [])
                if not rows:
                    continue
                pcc = float(np.mean([r.pcc for r in rows]))
                mae = float(np.mean([r.mae for r in rows]))
                atol = float(np.mean([r.atol for r in rows]))
                bytes_vals = [r.tile_bytes for r in rows if r.tile_bytes is not None]
                bytes_txt = ""
                if bytes_vals:
                    bytes_mean = float(np.mean(bytes_vals))
                    bytes_txt = f"  bytes={bytes_mean:,.0f}"
                line = (
                    f"  {comp.ljust(comp_w)} {fmt:>5}  "
                    f"pcc={paint(f'{pcc: .5f}', color_pcc(pcc))}  "
                    f"mae={paint(f'{mae:.3e}', color_err(mae))}  "
                    f"atol={paint(f'{atol:.3e}', color_err(atol))}"
                    f"{bytes_txt}"
                )
                print(line)
                table_lines.append(strip_ansi(line))

    if results_dir is not None and table_lines:
        (results_dir / "table.txt").write_text("\n".join(table_lines) + "\n", encoding="utf-8")

    return 0


if __name__ == "__main__":
    raise SystemExit(run())
