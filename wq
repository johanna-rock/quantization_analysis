#!/usr/bin/env python3
from __future__ import annotations

import argparse
import os
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import numpy as np
from tqdm import tqdm

from compression_algorithms import create_algorithm
from compression_algorithms.cache import CacheContext
from compression_algorithms.config import load_compression_config
from compression_algorithms.metrics import pearson_corr
from compression_algorithms.quantizer import Quantizer
from compression_algorithms.tile_utils import MIXED_TILE_FORMATS
from hf_model_utils import (
    _safe_repo_revision_key,
    _safe_tensor_key,
    build_model_index,
    fp32_tensor_cache_dir,
    load_tensor_fp32,
    resolve_format_list,
    resolve_selected_tensors,
)
from quantization_formats import SUPPORTED_FORMATS


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        prog="wq",
        description="Weight quantization analyzer for Hugging Face checkpoints.",
    )
    parser.add_argument("repo_or_url", help="Hugging Face model repo/URL.")
    parser.add_argument(
        "filter_query",
        nargs="*",
        help="Optional filter: substring, or dotted torch-style prefix path.",
    )
    parser.add_argument("--revision", default="main", help="Hugging Face revision (default: main).")
    parser.add_argument(
        "--cache-dir",
        default="data/hf-cache",
        help="Shared local cache for downloaded/dequantized tensors (default: data/hf-cache).",
    )
    parser.add_argument(
        "--limit", type=int, default=None, help="Optional max matched tensors."
    )
    parser.add_argument(
        "--backend",
        choices=["emulation", "ttnn"],
        default="emulation",
        help="Quantization backend for BFP formats (default: emulation).",
    )
    parser.add_argument(
        "--compression-config",
        type=str,
        default=None,
        help="Path to a JSON compression config file (default: none).",
    )
    parser.add_argument(
        "--recompute",
        action="store_true",
        help="Recompute and overwrite cached quantized tensors.",
    )
    parser.add_argument(
        "--summary",
        action="store_true",
        help="Print the aggregate summary (default: off).",
    )
    return parser.parse_args()


def _tensor_meta_str(x: np.ndarray) -> str:
    x = np.asarray(x, dtype=np.float32)
    return f"shape={tuple(x.shape)} min={np.min(x):.3e} mean={np.mean(x):.3e} max={np.max(x):.3e}"


COLOR_ENABLED = sys.stdout.isatty() and os.getenv("TERM", "") != "dumb" and not os.getenv("NO_COLOR")
COLORS = {
    "reset": "\033[0m",
    "title": "\033[1;37m",
    "muted": "\033[90m",
    "good": "\033[92m",
    "mid": "\033[93m",
    "bad": "\033[91m",
    "cyan": "\033[96m",
}


def paint(text: str, color: str) -> str:
    if not COLOR_ENABLED:
        return text
    return f"{COLORS[color]}{text}{COLORS['reset']}"


def color_pcc(v: float) -> str:
    if v >= 0.999:
        return "good"
    if v >= 0.99:
        return "mid"
    return "bad"


def color_err(v: float) -> str:
    if v <= 1e-4:
        return "good"
    if v <= 1e-3:
        return "mid"
    return "bad"


@dataclass
class Row:
    fmt: str
    compression: str
    pcc: float
    mae: float
    atol: float
    tile_counts: dict[str, int] | None = None
    tile_bytes: float | None = None


def _import_ttnn_quiet():
    os.environ.setdefault("LOGURU_LEVEL", "ERROR")
    os.environ.setdefault("TTNN_LOG_LEVEL", "ERROR")
    os.environ.setdefault("TT_METAL_LOGGER_LEVEL", "ERROR")
    try:
        import ttnn  # pylint: disable=import-outside-toplevel
    except ModuleNotFoundError as exc:
        raise RuntimeError("TTNN backend requires `ttnn` in the active Python environment.") from exc
    return ttnn


def _build_hierarchy(tensor_names: list[str]) -> dict:
    root: dict = {}
    for name in sorted(tensor_names):
        node = root
        for part in name.split("."):
            node = node.setdefault(part, {})
    return root


def _count_leaves(node: dict) -> int:
    if not node:
        return 1
    return sum(_count_leaves(child) for child in node.values())


def _render_hierarchy_lines(node: dict, prefix: str = "") -> list[str]:
    lines: list[str] = []
    items = sorted(node.items(), key=lambda kv: kv[0])
    for i, (name, child) in enumerate(items):
        is_last = i == len(items) - 1
        branch = "└── " if is_last else "├── "
        count = _count_leaves(child)
        label = f"{name} {paint(f'({count})', 'muted') if count > 1 else ''}".rstrip()
        lines.append(f"{prefix}{branch}{label}")
        if child:
            ext = "    " if is_last else "│   "
            lines.extend(_render_hierarchy_lines(child, prefix + ext))
    return lines


def _print_hierarchy(tensor_names: list[str]) -> None:
    tree = _build_hierarchy(tensor_names)
    print(paint("Hierarchy", "title"))
    for line in _render_hierarchy_lines(tree):
        print(f"  {paint(line, 'muted')}")
    print()


def run() -> int:
    args = parse_args()
    run_tag = datetime.now().strftime("%Y%m%d-%H%M%S")

    config = load_compression_config(args.compression_config)
    selected_algo = create_algorithm(config.algorithm, config.params)
    baseline = create_algorithm("none", {})
    algorithms = [baseline] if selected_algo.name == "none" else [baseline, selected_algo]

    filter_query = " ".join(args.filter_query).strip() or None
    formats = resolve_format_list(config.quantization_formats, SUPPORTED_FORMATS)

    index = build_model_index(repo_or_url=args.repo_or_url, revision=args.revision, cache_dir=args.cache_dir)
    tensor_names = resolve_selected_tensors(index, filter_query)
    if args.limit is not None:
        tensor_names = tensor_names[: max(0, args.limit)]
    if not tensor_names:
        print("No tensors matched.", file=sys.stderr)
        return 1

    ttnn = None
    if args.backend == "ttnn":
        try:
            ttnn = _import_ttnn_quiet()
        except Exception as exc:
            print(f"error: {exc}", file=sys.stderr)
            return 1

    quantizer = Quantizer(backend=args.backend, ttnn=ttnn)

    compression_names = [algo.name for algo in algorithms]
    comp_w = max(len("COMP"), max((len(name) for name in compression_names), default=0))
    print(
        f"{paint(index.repo_id, 'title')} {paint('@', 'muted')}{paint(index.revision, 'cyan')} "
        f"{paint('-', 'muted')} {paint(str(len(tensor_names)), 'title')} {paint('tensors', 'muted')}"
    )
    print(f"{paint('formats:', 'muted')} {', '.join(formats)}")
    print(f"{paint('compression:', 'muted')} {', '.join(compression_names)}")
    print(f"{paint('backend:', 'muted')} {args.backend}")
    if args.compression_config:
        print(f"{paint('config:', 'muted')} {args.compression_config}")
    print()
    _print_hierarchy(tensor_names)

    processed_root = Path("data/processed") / _safe_repo_revision_key(index.repo_id, index.revision)
    aggregate: dict[tuple[str, str], list[Row]] = {}

    total_evals = len(tensor_names) * sum(algo.expected_evals(formats) for algo in algorithms)
    pbar = tqdm(total=total_evals, desc="Evaluating", unit="eval")

    for tensor_name in tensor_names:
        cache_file = fp32_tensor_cache_dir(index) / f"{_safe_tensor_key(tensor_name)}.npy"
        if cache_file.exists():
            print(f"{paint('cache:', 'muted')} fp32 hit ({cache_file})")
        else:
            print(f"{paint('cache:', 'muted')} fp32 miss -> loading from HF cache/download")
        x = load_tensor_fp32(index, tensor_name)
        xf = np.asarray(x, dtype=np.float32)
        print(paint(tensor_name, "title"))
        print(f"  {paint(_tensor_meta_str(xf), 'muted')}")

        cache_ctx = CacheContext(
            root=processed_root,
            tensor_name=tensor_name,
            backend=args.backend,
            recompute=args.recompute,
            run_tag=run_tag,
        )

        rows_by_comp: dict[str, list[Row]] = {}
        for algo in algorithms:
            results = algo.run(xf=xf, formats=formats, quantizer=quantizer, cache=cache_ctx)
            for res in results:
                diff = np.abs(xf - res.y)
                mae = float(np.mean(diff))
                atol = float(np.max(diff))
                pcc = pearson_corr(xf, res.y)
                row = Row(
                    fmt=res.fmt,
                    compression=res.compression,
                    pcc=pcc,
                    mae=mae,
                    atol=atol,
                    tile_counts=res.tile_counts,
                    tile_bytes=res.tile_bytes,
                )
                rows_by_comp.setdefault(res.compression, []).append(row)
                aggregate.setdefault((res.compression, res.fmt), []).append(row)
                pbar.update(1)

        for comp in compression_names:
            rows = rows_by_comp.get(comp, [])
            if not rows:
                continue
            fmt_w = max(len(r.fmt) for r in rows)
            if comp in {"mixed-tile-greedy", "mixed-tile-random"}:
                count_widths = {k: len(k.upper()) for k in MIXED_TILE_FORMATS}
                bytes_w = len("BYTES")
                for r in rows:
                    counts = r.tile_counts or {}
                    for k in MIXED_TILE_FORMATS:
                        count_widths[k] = max(count_widths[k], len(str(counts.get(k, 0))))
                    if r.tile_bytes is not None:
                        bytes_w = max(bytes_w, len(f"{r.tile_bytes:,.0f}"))
                count_hdr = "  ".join(k.upper().rjust(count_widths[k]) for k in MIXED_TILE_FORMATS)
                print(
                    f"  {paint('COMP'.ljust(comp_w), 'muted')}  {paint('FORMAT'.ljust(fmt_w), 'muted')}  "
                    f"{paint('PCC', 'muted')}      {paint('MAE', 'muted')}      {paint('ATOL', 'muted')}  "
                    f"{paint(count_hdr, 'muted')}  {paint('BYTES'.rjust(bytes_w), 'muted')}"
                )
                for r in rows:
                    pcc_txt = f"{r.pcc: .5f}"
                    mae_txt = f"{r.mae:.3e}"
                    atol_txt = f"{r.atol:.3e}"
                    counts = r.tile_counts or {}
                    counts_txt = "  ".join(str(counts.get(k, 0)).rjust(count_widths[k]) for k in MIXED_TILE_FORMATS)
                    bytes_txt = f"{(r.tile_bytes or 0.0):,.0f}".rjust(bytes_w)
                    print(
                        f"  {r.compression.ljust(comp_w)}  {r.fmt.ljust(fmt_w)}  "
                        f"{paint(pcc_txt, color_pcc(r.pcc))}  "
                        f"{paint(mae_txt, color_err(r.mae))}  "
                        f"{paint(atol_txt, color_err(r.atol))}  "
                        f"{counts_txt}  {bytes_txt}"
                    )
            else:
                print(
                    f"  {paint('COMP'.ljust(comp_w), 'muted')}  {paint('FORMAT'.ljust(fmt_w), 'muted')}  "
                    f"{paint('PCC', 'muted')}      {paint('MAE', 'muted')}      {paint('ATOL', 'muted')}"
                )
                for r in rows:
                    pcc_txt = f"{r.pcc: .5f}"
                    mae_txt = f"{r.mae:.3e}"
                    atol_txt = f"{r.atol:.3e}"
                    print(
                        f"  {r.compression.ljust(comp_w)}  {r.fmt.ljust(fmt_w)}  "
                        f"{paint(pcc_txt, color_pcc(r.pcc))}  "
                        f"{paint(mae_txt, color_err(r.mae))}  "
                        f"{paint(atol_txt, color_err(r.atol))}"
                    )
            print()

    pbar.close()
    if args.summary:
        print(paint("Summary (mean across matched tensors)", "title"))
        for comp in compression_names:
            if comp in {"mixed-tile-greedy", "mixed-tile-random"}:
                fmt_list = ["MIXED"]
            else:
                fmt_list = [fmt.upper() for fmt in formats]
            for fmt in fmt_list:
                rows = aggregate.get((comp, fmt), [])
                if not rows:
                    continue
                pcc = float(np.mean([r.pcc for r in rows]))
                mae = float(np.mean([r.mae for r in rows]))
                atol = float(np.mean([r.atol for r in rows]))
                bytes_vals = [r.tile_bytes for r in rows if r.tile_bytes is not None]
                bytes_txt = ""
                if bytes_vals:
                    bytes_mean = float(np.mean(bytes_vals))
                    bytes_txt = f"  bytes={bytes_mean:,.0f}"
                print(
                    f"  {comp.ljust(comp_w)} {fmt:>5}  "
                    f"pcc={paint(f'{pcc: .5f}', color_pcc(pcc))}  "
                    f"mae={paint(f'{mae:.3e}', color_err(mae))}  "
                    f"atol={paint(f'{atol:.3e}', color_err(atol))}"
                    f"{bytes_txt}"
                )

    return 0


if __name__ == "__main__":
    raise SystemExit(run())
